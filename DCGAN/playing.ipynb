{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\david\\anaconda3\\envs\\machine_learning\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import argparse\n",
    "import os\n",
    "import random\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as vutils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = os.path.join('C:/Users/david/Documents/Python/data','human_face')\n",
    "batchsize = 32\n",
    "workers = 8\n",
    "nz = 100\n",
    "\n",
    "# folder dataset\n",
    "human_dataset = dset.ImageFolder(root=data_path,\n",
    "                        transform=transforms.Compose([\n",
    "                            transforms.Resize(64),\n",
    "                            transforms.CenterCrop(64),\n",
    "                            transforms.ToTensor(),\n",
    "                            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "                        ]))\n",
    "\n",
    "assert human_dataset\n",
    "human_dataloader = torch.utils.data.DataLoader(human_dataset, batch_size=batchsize,\n",
    "                                        shuffle=True, num_workers=int(workers))\n",
    "\n",
    "# Grab sample. Depending on dataset, may take a while\n",
    "sample = next(iter(human_dataloader))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size_measure = sample.size(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_noise = torch.randn(batchsize, nz, 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_to_vector(img, nz):\n",
    "    preprocess = transforms.Compose([\n",
    "        transforms.Grayscale(),\n",
    "        transforms.CenterCrop(int(math.sqrt(nz)))\n",
    "    ])\n",
    "    vector = preprocess(img)\n",
    "    # vector = torch.unsqueeze(vector,1)\n",
    "    vector = torch.flatten(vector,start_dim=1,end_dim=3)\n",
    "    return vector[:,:,None,None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector = image_to_vector(sample,nz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, ngpu,nz,nc,ngf):\n",
    "        super(Generator, self).__init__()\n",
    "        self.ngpu = ngpu\n",
    "        self.nz = nz\n",
    "        self.nc = nc\n",
    "        self.ngf = ngf\n",
    "        self.main = nn.Sequential(\n",
    "            # input is Z, going into a convolution\n",
    "            nn.ConvTranspose2d(     self.nz, self.ngf * 8, 4, 1, 0, bias=False),\n",
    "            nn.BatchNorm2d(self.ngf * 8),\n",
    "            nn.ReLU(True),\n",
    "            \n",
    "            # state size. (ngf*8) x 4 x 4\n",
    "            nn.ConvTranspose2d(self.ngf * 8, self.ngf * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(self.ngf * 4),\n",
    "            nn.ReLU(True),\n",
    "            \n",
    "            # state size. (ngf*4) x 8 x 8\n",
    "            nn.ConvTranspose2d(self.ngf * 4, self.ngf * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(self.ngf * 2),\n",
    "            nn.ReLU(True),\n",
    "            \n",
    "            # state size. (ngf*2) x 16 x 16\n",
    "            nn.ConvTranspose2d(self.ngf * 2,     self.ngf, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(self.ngf),\n",
    "            nn.ReLU(True),\n",
    "\n",
    "            # state size. (ngf) x 32 x 32\n",
    "            nn.ConvTranspose2d( self.ngf, nc, 4, 2, 1, bias=False),\n",
    "            nn.Tanh()\n",
    "            # state size. (nc) x 64 x 64\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        if input.is_cuda and self.ngpu > 1:\n",
    "            output = nn.parallel.data_parallel(self.main, input, range(self.ngpu))\n",
    "        else:\n",
    "            output = self.main(input)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "netG = Generator(1,nz,3,64)\n",
    "# netG.apply(weights_init)\n",
    "netG.load_state_dict(torch.load('C:/Users/david/Documents/Python/DCGAN/output/anime_face/generator_chkpts/netG_epoch_34.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Cannot handle this data type: (1, 1, 100), |u1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\david\\anaconda3\\envs\\machine_learning\\lib\\site-packages\\PIL\\Image.py:2928\u001b[0m, in \u001b[0;36mfromarray\u001b[1;34m(obj, mode)\u001b[0m\n\u001b[0;32m   2927\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 2928\u001b[0m     mode, rawmode \u001b[39m=\u001b[39m _fromarray_typemap[typekey]\n\u001b[0;32m   2929\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n",
      "\u001b[1;31mKeyError\u001b[0m: ((1, 1, 100), '|u1')",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\david\\Documents\\Python\\DCGAN\\playing.ipynb Cell 9\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/david/Documents/Python/DCGAN/playing.ipynb#X12sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m fake \u001b[39m=\u001b[39m netG(fixed_noise)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/david/Documents/Python/DCGAN/playing.ipynb#X12sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m vutils\u001b[39m.\u001b[39;49msave_image(fixed_noise,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/david/Documents/Python/DCGAN/playing.ipynb#X12sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m         \u001b[39m'\u001b[39;49m\u001b[39mnoise_input.png\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/david/Documents/Python/DCGAN/playing.ipynb#X12sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m         normalize\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/david/Documents/Python/DCGAN/playing.ipynb#X12sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m vutils\u001b[39m.\u001b[39msave_image(fake\u001b[39m.\u001b[39mdetach(),\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/david/Documents/Python/DCGAN/playing.ipynb#X12sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mnoise_output.png\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/david/Documents/Python/DCGAN/playing.ipynb#X12sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m         normalize\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/david/Documents/Python/DCGAN/playing.ipynb#X12sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m fake \u001b[39m=\u001b[39m netG(vector)\n",
      "File \u001b[1;32mc:\\Users\\david\\anaconda3\\envs\\machine_learning\\lib\\site-packages\\torch\\autograd\\grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[0;32m     25\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m     26\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclone():\n\u001b[1;32m---> 27\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\david\\anaconda3\\envs\\machine_learning\\lib\\site-packages\\torchvision\\utils.py:155\u001b[0m, in \u001b[0;36msave_image\u001b[1;34m(tensor, fp, format, **kwargs)\u001b[0m\n\u001b[0;32m    153\u001b[0m \u001b[39m# Add 0.5 after unnormalizing to [0, 255] to round to nearest integer\u001b[39;00m\n\u001b[0;32m    154\u001b[0m ndarr \u001b[39m=\u001b[39m grid\u001b[39m.\u001b[39mmul(\u001b[39m255\u001b[39m)\u001b[39m.\u001b[39madd_(\u001b[39m0.5\u001b[39m)\u001b[39m.\u001b[39mclamp_(\u001b[39m0\u001b[39m, \u001b[39m255\u001b[39m)\u001b[39m.\u001b[39mpermute(\u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m, \u001b[39m0\u001b[39m)\u001b[39m.\u001b[39mto(\u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m, torch\u001b[39m.\u001b[39muint8)\u001b[39m.\u001b[39mnumpy()\n\u001b[1;32m--> 155\u001b[0m im \u001b[39m=\u001b[39m Image\u001b[39m.\u001b[39;49mfromarray(ndarr)\n\u001b[0;32m    156\u001b[0m im\u001b[39m.\u001b[39msave(fp, \u001b[39mformat\u001b[39m\u001b[39m=\u001b[39m\u001b[39mformat\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\david\\anaconda3\\envs\\machine_learning\\lib\\site-packages\\PIL\\Image.py:2930\u001b[0m, in \u001b[0;36mfromarray\u001b[1;34m(obj, mode)\u001b[0m\n\u001b[0;32m   2928\u001b[0m         mode, rawmode \u001b[39m=\u001b[39m _fromarray_typemap[typekey]\n\u001b[0;32m   2929\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m-> 2930\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mCannot handle this data type: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m typekey) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n\u001b[0;32m   2931\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   2932\u001b[0m     rawmode \u001b[39m=\u001b[39m mode\n",
      "\u001b[1;31mTypeError\u001b[0m: Cannot handle this data type: (1, 1, 100), |u1"
     ]
    }
   ],
   "source": [
    "fake = netG(fixed_noise)\n",
    "vutils.save_image(fake.detach(),\n",
    "        'noise_output.png',\n",
    "        normalize=True)\n",
    "        \n",
    "fake = netG(vector)\n",
    "vutils.save_image(vector,\n",
    "        'vector_input.png',\n",
    "        normalize=True)\n",
    "vutils.save_image(fake.detach(),\n",
    "        'vector_output.png',\n",
    "        normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('machine_learning')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "674291698da69398d09eaaff8f2d16745172857c9fb6a1cd3dbf75eeb9b95a3a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
